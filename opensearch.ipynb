{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to:\n",
    "\n",
    "1. Connect to a local Dockerized OpenSearch instance.\n",
    "2. Create and configure an OpenSearch index for vector storage.\n",
    "3. Use LangChain to split large Python code into manageable chunks.\n",
    "4. Generate embeddings using Cohere's embed-english-v3 model from Amazon Bedrock.\n",
    "5. Store the embeddings and text chunks in OpenSearch.\n",
    "6. Execute vector searches based on different queries and analyze search accuracy.\n",
    "7. Optimize retrieval by addressing common mistakes.\n",
    "\n",
    "Ensure you have:\n",
    "- A local Dockerized OpenSearch instance running on the default port (9200).\n",
    "- Valid AWS credentials configured for accessing Amazon Bedrock (for embeddings).\n",
    "- The necessary Python dependencies installed (opensearch-py, langchain, boto3, etc.).\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Imports and Preliminary Setup\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import boto3\n",
    "import uuid\n",
    "import requests\n",
    "from typing import List\n",
    "\n",
    "# OpenSearch client from opensearch-py\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, helpers\n",
    "\n",
    "# LangChain text splitters\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import Language\n",
    "\n",
    "\n",
    "# import warnings\n",
    "# from urllib3.exceptions import InsecureRequestWarning\n",
    "\n",
    "# warnings.simplefilter(\"ignore\", InsecureRequestWarning)\n",
    "\n",
    "\n",
    "# Adjust these settings if needed\n",
    "OPENSEARCH_HOST = \"localhost\"\n",
    "OPENSEARCH_PORT = 9200\n",
    "OPENSEARCH_USER = \"admin\"  # If you have authentication\n",
    "OPENSEARCH_PASS = \"*asca9schasihca0CE\"  # Replace if you've set a different password\n",
    "\n",
    "# For Amazon Bedrock usage:\n",
    "REGION_NAME = \"ap-southeast-2\"  # or your region\n",
    "BEDROCK_MODEL_ID = \"cohere.embed-english-v3\"\n",
    "\n",
    "# Index name in OpenSearch\n",
    "INDEX_NAME = \"code-embeddings-index\"\n",
    "\n",
    "sample_file_path = \"src/sample_rename.py\"\n",
    "\n",
    "# Additional configurations\n",
    "CHUNK_SIZE = 200\n",
    "CHUNK_OVERLAP = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marcu\\.conda\\envs\\MachineLearning\\Lib\\site-packages\\opensearchpy\\connection\\http_requests.py:157: UserWarning: Connecting to https://localhost:9200 using SSL with verify_certs=False is insecure.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to OpenSearch. Cluster health: yellow\n",
      "Created index 'code-embeddings-index'\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Connect to OpenSearch and Create Index If Needed\n",
    "\n",
    "# Create the OpenSearch client\n",
    "# If you have no auth, you might omit http_auth. Adjust verify_certs or use SSL as needed.\n",
    "auth = (OPENSEARCH_USER, OPENSEARCH_PASS)\n",
    "client = OpenSearch(\n",
    "    hosts=[{\"host\": OPENSEARCH_HOST, \"port\": OPENSEARCH_PORT}],\n",
    "    http_auth=auth,\n",
    "    use_ssl=True,\n",
    "    verify_certs=False,\n",
    "    connection_class=RequestsHttpConnection,\n",
    ")\n",
    "\n",
    "# Check connection\n",
    "try:\n",
    "    health = client.cluster.health()\n",
    "    print(\"Connected to OpenSearch. Cluster health:\", health[\"status\"])\n",
    "except Exception as e:\n",
    "    print(\"Failed to connect to OpenSearch:\", e)\n",
    "    raise e\n",
    "\n",
    "# Create index mapping for vector search\n",
    "index_body = {\n",
    "    \"settings\": {\n",
    "        \"index\": {\n",
    "            \"knn\": True,\n",
    "            \"number_of_shards\": 1,\n",
    "            \"number_of_replicas\": 0\n",
    "        },\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"keyword_analyzer\": {\n",
    "                    \"type\": \"custom\",\n",
    "                    \"tokenizer\": \"keyword\",\n",
    "                    \"filter\": [\"lowercase\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"text\": {\"type\": \"text\", \"analyzer\": \"keyword_analyzer\"},\n",
    "            \"embedding\": {\n",
    "                \"type\": \"knn_vector\",\n",
    "                \"dimension\": 1024,  # This is the dimension for Cohere embed-english-v3 (example dimension)\n",
    "                \"method\": {\n",
    "                    \"name\": \"hnsw\",\n",
    "                    \"space_type\": \"cosinesimil\",\n",
    "                    \"engine\": \"nmslib\",\n",
    "                    \"parameters\": {\n",
    "                        \"m\": 16,\n",
    "                        \"ef_construction\": 100\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "client.indices.delete(index=INDEX_NAME, ignore=[400, 404])\n",
    "# Create or update index\n",
    "if not client.indices.exists(index=INDEX_NAME):\n",
    "    client.indices.create(index=INDEX_NAME, body=index_body)\n",
    "    print(f\"Created index '{INDEX_NAME}'\")\n",
    "else:\n",
    "    print(f\"Index '{INDEX_NAME}' already exists.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks created: 362\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Read and Split Sample Python Code\n",
    "\n",
    "# Assume 'src/sample.py' is a file with ~2500 lines of example Python code\n",
    "# For demonstration, ensure you have this file in your local environment\n",
    "\n",
    "\n",
    "if not os.path.exists(sample_file_path):\n",
    "    raise FileNotFoundError(f\"Could not find the file: {sample_file_path}\")\n",
    "\n",
    "with open(sample_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    code_data = f.read()\n",
    "\n",
    "extra_splitters = ['\\nclass','\\ndef']\n",
    "# Use LangChain's RecursiveCharacterTextSplitter with Python-specific splitting\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    RecursiveCharacterTextSplitter.get_separators_for_language(Language.PYTHON).extend(extra_splitters),\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    ")\n",
    "chunks = splitter.split_text(code_data)\n",
    "print(f\"Number of chunks created: {len(chunks)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Set Up Amazon Bedrock for Cohere Embeddings\n",
    "\n",
    "#Read credentials from credentials.json\n",
    "with open(\"credentials.json\", \"r\") as f:\n",
    "    creds = json.load(f)   \n",
    "\n",
    "# Create a Bedrock runtime client with credentials as a parameter\n",
    "bedrock = boto3.client(\"bedrock-runtime\", region_name=REGION_NAME,\n",
    "    aws_access_key_id=creds.get(\"aws_access_key_id\"),\n",
    "    aws_secret_access_key=creds.get('aws_secret_access_key'),\n",
    "    aws_session_token=creds.get('aws_session_token')\n",
    ")\n",
    "def cohere_embed_texts(texts, model_id):\n",
    "    # texts is a list of strings\n",
    "    if not texts:\n",
    "        return []\n",
    "\n",
    "    # Prepare the request body for Cohere's embed-english-v3\n",
    "    body_dict = {\n",
    "        \"texts\": texts,                  # A list of strings\n",
    "        \"input_type\": \"search_document\"  # or \"query\" if you're embedding queries\n",
    "    }\n",
    "\n",
    "    response = bedrock.invoke_model(\n",
    "        modelId=model_id,\n",
    "        contentType=\"application/json\",\n",
    "        accept=\"application/json\",\n",
    "        body=json.dumps(body_dict)\n",
    "    )\n",
    "\n",
    "    resp_body = json.loads(response[\"body\"].read())\n",
    "    # For embed-english-v3, the model returns { \"embeddings\": [ [vector], [vector], ... ] }\n",
    "    # so you get an embedding for each text in the input.\n",
    "    embeddings = resp_body[\"embeddings\"]  # list of lists\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing completed. Bulk response: (362, [])\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Generate Embeddings for Each Chunk and Index into OpenSearch\n",
    "\n",
    "docs_to_index = []\n",
    "batch_size = 16\n",
    "all_chunks = chunks[:]  # Copy for manipulation\n",
    "\n",
    "while all_chunks:\n",
    "    batch = all_chunks[:batch_size]\n",
    "    all_chunks = all_chunks[batch_size:]\n",
    "\n",
    "    # Generate embeddings via Amazon Bedrock\n",
    "    batch_embeddings = cohere_embed_texts(batch, BEDROCK_MODEL_ID)\n",
    "\n",
    "    for text_chunk, embedding in zip(batch, batch_embeddings):\n",
    "        doc_id = str(uuid.uuid4())\n",
    "        doc_body = {\n",
    "            \"text\": text_chunk,\n",
    "            \"embedding\": embedding\n",
    "        }\n",
    "        docs_to_index.append({\n",
    "            \"_index\": INDEX_NAME,\n",
    "            \"_id\": doc_id,\n",
    "            \"_source\": doc_body\n",
    "        })\n",
    "\n",
    "# Use bulk helper to index all\n",
    "resp = helpers.bulk(client, docs_to_index)\n",
    "print(\"Indexing completed. Bulk response:\", resp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index refreshed and ready for search.\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Refresh Index and Prepare for Searches\n",
    "\n",
    "# Refresh so that newly indexed documents are available\n",
    "client.indices.refresh(index=INDEX_NAME)\n",
    "print(\"Index refreshed and ready for search.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'simple_queries' from 'src.test_queries' (c:\\Users\\marcu\\Documents\\2pi\\vectorsearch-test\\src\\test_queries.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Cell 8: Define Test Queries for Different Programming Tasks\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Cell: Import Queries\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtest_queries\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m simple_queries\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Confirm they're loaded\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(test_queries)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m queries:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'simple_queries' from 'src.test_queries' (c:\\Users\\marcu\\Documents\\2pi\\vectorsearch-test\\src\\test_queries.py)"
     ]
    }
   ],
   "source": [
    "# Cell 8: Define Test Queries for Different Programming Tasks\n",
    "\n",
    "# Cell: Import Queries\n",
    "\n",
    "from src.test_queries import simple_queries\n",
    "\n",
    "# Confirm they're loaded\n",
    "print(f\"Loaded {len(test_queries)} queries:\")\n",
    "for q in simple_queries:\n",
    "    print(\"-\", q[\"description\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Implement a Vector kNN Search in OpenSearch\n",
    "\n",
    "def vector_search_opensearch(query_text: str, k: int = 5):\n",
    "    \"\"\"\n",
    "    Perform a vector search in OpenSearch using the query text embedded by Cohere.\n",
    "    Return top k results.\n",
    "    \"\"\"\n",
    "    # Embed query\n",
    "    query_embedding = cohere_embed_texts([query_text], BEDROCK_MODEL_ID)[0]\n",
    "\n",
    "    # Construct kNN query\n",
    "    search_body = {\n",
    "        \"size\": k,\n",
    "        \"query\": {\n",
    "            \"knn\": {\n",
    "                \"embedding\": {\n",
    "                    \"vector\": query_embedding,\n",
    "                    \"k\": k\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = client.search(index=INDEX_NAME, body=search_body)\n",
    "        hits = response[\"hits\"][\"hits\"]\n",
    "        return hits\n",
    "    except Exception as e:\n",
    "        print(f\"Search error: {e}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Test 1: Parsing imports with resolvers\n",
      "\n",
      "**Query**: Which function can find all the import statements in Python code?\n",
      "\n",
      "**Expected Keywords**: import, resolver, find_imports, ImportResolver\n",
      "\n",
      "### Rank 1 | Score: 0.7755966\n",
      "\n",
      "```python\n",
      "def parse_python_imports(code):\n",
      "    lines = code.split(\"\\n\")\n",
      "    modules = []\n",
      "    for l in lines:\n",
      "        if l.startswith(\"import \") or l.startswith(\"from \"):\n",
      "            modules.append(l)\n",
      "```\n",
      "\n",
      "### Rank 2 | Score: 0.74791616\n",
      "\n",
      "```python\n",
      "def nova_import(code_block):\n",
      "    found = []\n",
      "    lines = code_block.splitlines()\n",
      "    for line in lines:\n",
      "        if 'import ' in line:\n",
      "            parts = line.strip().split()\n",
      "```\n",
      "\n",
      "### Rank 3 | Score: 0.73969615\n",
      "\n",
      "```python\n",
      "if 'import' in parts:\n",
      "                idx = parts.index('import')\n",
      "                if idx + 1 < len(parts):\n",
      "                    found.append(parts[idx+1])\n",
      "        if 'from ' in line:\n",
      "```\n",
      "\n",
      "## Test 2: Reverse words in a string\n",
      "\n",
      "**Query**: How do I reverse the words in a sentence for string manipulation?\n",
      "\n",
      "**Expected Keywords**: reverse_words_in_string, split, string manipulation\n",
      "\n",
      "### Rank 1 | Score: 0.7143035\n",
      "\n",
      "```python\n",
      "def elegant_reverse(s):\n",
      "    words = s.split()\n",
      "    return \" \".join(word[::-1] for word in words)\n",
      "\n",
      "def fluent_topk(lst, k):\n",
      "    freq = Counter(lst)\n",
      "    return [x for x, _ in freq.most_common(k)]\n",
      "```\n",
      "\n",
      "### Rank 2 | Score: 0.7000158\n",
      "\n",
      "```python\n",
      "for end in range(start, len(s)):\n",
      "            substring = s[start:end+1]\n",
      "            if substring == substring[::-1]:\n",
      "                part.append(substring)\n",
      "                backtrack(end+1)\n",
      "```\n",
      "\n",
      "### Rank 3 | Score: 0.6930474\n",
      "\n",
      "```python\n",
      "n //= base\n",
      "    if num < 0:\n",
      "        result.append('-')\n",
      "    return ''.join(reversed(result))\n",
      "```\n",
      "\n",
      "## Test 3: Simple linear regression\n",
      "\n",
      "**Query**: I need a basic function for linear regression that can train and predict values.\n",
      "\n",
      "**Expected Keywords**: BasicRegressionModel, fit, predict\n",
      "\n",
      "### Rank 1 | Score: 0.7404915\n",
      "\n",
      "```python\n",
      "self.w -= self.lr * dw / n\n",
      "            self.b -= self.lr * db / n\n",
      "    def predict_proba(self, X):\n",
      "        return [self.sigmoid(self.w*xi + self.b) for xi in X]\n",
      "    def predict(self, X):\n",
      "```\n",
      "\n",
      "### Rank 2 | Score: 0.7174916\n",
      "\n",
      "```python\n",
      "return [1 if p >= 0.5 else 0 for p in self.predict_proba(X)]\n",
      "```\n",
      "\n",
      "### Rank 3 | Score: 0.7072806\n",
      "\n",
      "```python\n",
      "return value in self.data\n",
      "```\n",
      "\n",
      "## Test 4: SQLite insertion and retrieval\n",
      "\n",
      "**Query**: Which snippet shows a method to insert and select rows from an SQLite database?\n",
      "\n",
      "**Expected Keywords**: INSERT INTO, SELECT, sqlite3, simple_database_insert, simple_database_fetch_all\n",
      "\n",
      "### Rank 1 | Score: 0.7553623\n",
      "\n",
      "```python\n",
      "def keen_insert(db_path, name, value):\n",
      "    conn = sqlite3.connect(db_path)\n",
      "    c = conn.cursor()\n",
      "    c.execute('INSERT INTO items (name, value) VALUES (?, ?)', (name, value))\n",
      "    conn.commit()\n",
      "```\n",
      "\n",
      "### Rank 2 | Score: 0.7173858\n",
      "\n",
      "```python\n",
      "def mosaic_update(db_path, item_id, new_value):\n",
      "    conn = sqlite3.connect(db_path)\n",
      "    c = conn.cursor()\n",
      "    c.execute('UPDATE items SET value = ? WHERE id = ?', (new_value, item_id))\n",
      "```\n",
      "\n",
      "### Rank 3 | Score: 0.71169925\n",
      "\n",
      "```python\n",
      "def luminous_fetch(db_path):\n",
      "    conn = sqlite3.connect(db_path)\n",
      "    c = conn.cursor()\n",
      "    c.execute('SELECT id, name, value FROM items')\n",
      "    rows = c.fetchall()\n",
      "    conn.close()\n",
      "    return rows\n",
      "```\n",
      "\n",
      "## Test 5: Random BST creation\n",
      "\n",
      "**Query**: How do I build a random BST with a specified number of nodes?\n",
      "\n",
      "**Expected Keywords**: random_bst, NodeTree, insert_into_bst\n",
      "\n",
      "### Rank 1 | Score: 0.74554724\n",
      "\n",
      "```python\n",
      "def harmony_bst(num_nodes, value_range=(0,100)):\n",
      "    values = [random.randint(value_range[0], value_range[1]) for _ in range(num_nodes)]\n",
      "    root = None\n",
      "    for v in values:\n",
      "```\n",
      "\n",
      "### Rank 2 | Score: 0.71046203\n",
      "\n",
      "```python\n",
      "def random_permutation(n):\n",
      "    arr = list(range(n))\n",
      "    random.shuffle(arr)\n",
      "    return arr\n",
      "```\n",
      "\n",
      "### Rank 3 | Score: 0.68572986\n",
      "\n",
      "```python\n",
      "self.w_xh = [[random.uniform(-0.1, 0.1) for _ in range(hidden_size)] for __ in range(input_size)]\n",
      "```\n",
      "\n",
      "## Test 6: Web scraping with HTML parsing\n",
      "\n",
      "**Query**: Where is the function that scrapes a webpage and returns the page title and links?\n",
      "\n",
      "**Expected Keywords**: simple_web_scraper, parse_html_title, parse_html_links\n",
      "\n",
      "### Rank 1 | Score: 0.7152056\n",
      "\n",
      "```python\n",
      "def parse_url_parameters(url):\n",
      "    query = urllib.parse.urlparse(url).query\n",
      "    return dict(urllib.parse.parse_qsl(query))\n",
      "```\n",
      "\n",
      "### Rank 2 | Score: 0.70768565\n",
      "\n",
      "```python\n",
      "def pensive_headers(url):\n",
      "    req = urllib.request.Request(url, method='HEAD')\n",
      "    with urllib.request.urlopen(req) as response:\n",
      "        return response.info()\n",
      "```\n",
      "\n",
      "### Rank 3 | Score: 0.6989496\n",
      "\n",
      "```python\n",
      "return list(found)\n",
      "```\n",
      "\n",
      "## Test 7: Random string generation\n",
      "\n",
      "**Query**: I want to generate a random alphanumeric string of a given length.\n",
      "\n",
      "**Expected Keywords**: random_alphanumeric_string, random_hex_string\n",
      "\n",
      "### Rank 1 | Score: 0.779092\n",
      "\n",
      "```python\n",
      "def random_lower_upper_string(n):\n",
      "    s = []\n",
      "    for _ in range(n):\n",
      "        c = random.choice('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ')\n",
      "        s.append(c)\n",
      "    return \"\".join(s)\n",
      "```\n",
      "\n",
      "### Rank 2 | Score: 0.76461357\n",
      "\n",
      "```python\n",
      "def xenial(length):\n",
      "    chars = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789'\n",
      "    return ''.join(random.choice(chars) for _ in range(length))\n",
      "```\n",
      "\n",
      "### Rank 3 | Score: 0.7555562\n",
      "\n",
      "```python\n",
      "def violet(length):\n",
      "    chars = '0123456789abcdef'\n",
      "    return ''.join(random.choice(chars) for _ in range(length))\n",
      "```\n",
      "\n",
      "## Test 8: Basic XOR encryption\n",
      "\n",
      "**Query**: Which function implements an XOR cipher for strings with a numeric key?\n",
      "\n",
      "**Expected Keywords**: xor_cipher, encryption, string XOR\n",
      "\n",
      "### Rank 1 | Score: 0.75205487\n",
      "\n",
      "```python\n",
      "def vortex_xor(s, key=42):\n",
      "    return ''.join(chr(ord(ch) ^ key) for ch in s)\n",
      "\n",
      "def wondrous_euler(n_terms=10):\n",
      "    return sum(1 / math.factorial(i) for i in range(n_terms))\n",
      "```\n",
      "\n",
      "### Rank 2 | Score: 0.7449581\n",
      "\n",
      "```python\n",
      "def numeric_range(start, end):\n",
      "    return list(range(start, end))\n",
      "\n",
      "def crypt_shift_string(s, shift):\n",
      "    return \"\".join(chr((ord(ch) + shift) % 256) for ch in s)\n",
      "```\n",
      "\n",
      "### Rank 3 | Score: 0.7087835\n",
      "\n",
      "```python\n",
      "def apply_mask_to_string(s, mask):\n",
      "    r = []\n",
      "    for ch, m in zip(s, mask):\n",
      "        if m == '1':\n",
      "            r.append(ch.upper())\n",
      "        else:\n",
      "            r.append(ch.lower())\n",
      "```\n",
      "\n",
      "## Test 9: K-Means clustering\n",
      "\n",
      "**Query**: I need a class for k-means clustering on 2D points. Where can I find it?\n",
      "\n",
      "**Expected Keywords**: BasicKMeans, fit, predict, centroids\n",
      "\n",
      "### Rank 1 | Score: 0.67867947\n",
      "\n",
      "```python\n",
      "class evergreen:\n",
      "    def __init__(self):\n",
      "        self.classes = []\n",
      "        self.log_priors = {}\n",
      "        self.word_counts = {}\n",
      "        self.class_word_totals = {}\n",
      "    def fit(self, X, y):\n",
      "```\n",
      "\n",
      "### Rank 2 | Score: 0.67596096\n",
      "\n",
      "```python\n",
      "def determinant_2x2(self, mat):\n",
      "        return mat[0][0]*mat[1][1] - mat[0][1]*mat[1][0]\n",
      "```\n",
      "\n",
      "### Rank 3 | Score: 0.67438376\n",
      "\n",
      "```python\n",
      "self.classes = list(set(y))\n",
      "        for c in self.classes:\n",
      "            self.log_priors[c] = math.log(y.count(c)/len(y))\n",
      "            self.word_counts[c] = defaultdict(int)\n",
      "```\n",
      "\n",
      "## Test 10: SHA-256 string hashing\n",
      "\n",
      "**Query**: How do I compute a SHA256 hash of a given string?\n",
      "\n",
      "**Expected Keywords**: hash_string_sha256, hashlib, SHA256\n",
      "\n",
      "### Rank 1 | Score: 0.72381985\n",
      "\n",
      "```python\n",
      "def generate_file_md5(path):\n",
      "    hasher = hashlib.md5()\n",
      "    for chunk in read_file_in_chunks(path, 4096):\n",
      "        hasher.update(chunk)\n",
      "    return hasher.hexdigest()\n",
      "```\n",
      "\n",
      "### Rank 2 | Score: 0.7228579\n",
      "\n",
      "```python\n",
      "def ribbon(s):\n",
      "    return hashlib.sha256(s.encode('utf-8')).hexdigest()\n",
      "```\n",
      "\n",
      "### Rank 3 | Score: 0.71488494\n",
      "\n",
      "```python\n",
      "def numeric_range(start, end):\n",
      "    return list(range(start, end))\n",
      "\n",
      "def crypt_shift_string(s, shift):\n",
      "    return \"\".join(chr((ord(ch) + shift) % 256) for ch in s)\n",
      "```\n",
      "\n",
      "## Test 11: JSON string parsing\n",
      "\n",
      "**Query**: Which snippet can parse JSON strings and turn them into Python objects?\n",
      "\n",
      "**Expected Keywords**: parse_json_string, json.loads\n",
      "\n",
      "### Rank 1 | Score: 0.766541\n",
      "\n",
      "```python\n",
      "def save_json_file(path, data):\n",
      "    with open(path, 'w', encoding='utf-8') as f:\n",
      "        json.dump(data, f)\n",
      "```\n",
      "\n",
      "### Rank 2 | Score: 0.76065886\n",
      "\n",
      "```python\n",
      "def load_json_file(path):\n",
      "    with open(path, 'r', encoding='utf-8') as f:\n",
      "        return json.load(f)\n",
      "```\n",
      "\n",
      "### Rank 3 | Score: 0.7337921\n",
      "\n",
      "```python\n",
      "def parse_integers_in_brackets(s):\n",
      "    pattern = r'\\[(\\d+)\\]'\n",
      "    return [int(x) for x in re.findall(pattern, s)]\n",
      "```\n",
      "\n",
      "## Test 12: Simple socket server\n",
      "\n",
      "**Query**: I want a function that sets up a basic TCP server to echo data in uppercase.\n",
      "\n",
      "**Expected Keywords**: simple_socket_server, socket, listen, accept\n",
      "\n",
      "### Rank 1 | Score: 0.75390565\n",
      "\n",
      "```python\n",
      "self.socket.connect((host, port))\n",
      "    def send(self, data):\n",
      "        if self.socket:\n",
      "            self.socket.sendall(data.encode('utf-8'))\n",
      "    def receive(self):\n",
      "        if self.socket:\n",
      "```\n",
      "\n",
      "### Rank 2 | Score: 0.73485744\n",
      "\n",
      "```python\n",
      "conn.sendall(data.upper())\n",
      "    conn.close()\n",
      "    srv.close()\n",
      "```\n",
      "\n",
      "### Rank 3 | Score: 0.7263913\n",
      "\n",
      "```python\n",
      "def simple_smtp_mock_server(host, port):\n",
      "    srv = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
      "    srv.bind((host, port))\n",
      "    srv.listen(1)\n",
      "    conn, addr = srv.accept()\n",
      "```\n",
      "\n",
      "## Test 13: BFS graph traversal\n",
      "\n",
      "**Query**: Show me the BFS graph code that returns nodes in breadth-first order.\n",
      "\n",
      "**Expected Keywords**: BFSGraph, bfs, deque, adj\n",
      "\n",
      "### Rank 1 | Score: 0.70637953\n",
      "\n",
      "```python\n",
      "if not root:\n",
      "            return 0\n",
      "        return 1 + self.count_nodes(root.left) + self.count_nodes(root.right)\n",
      "    def level_order(self, root):\n",
      "        results = []\n",
      "        if not root:\n",
      "```\n",
      "\n",
      "### Rank 2 | Score: 0.6963228\n",
      "\n",
      "```python\n",
      "class cascade:\n",
      "    def __init__(self):\n",
      "        self.adj = defaultdict(list)\n",
      "    def add_edge(self, u, v):\n",
      "        self.adj[u].append(v)\n",
      "        self.adj[v].append(u)\n",
      "    def bfs(self, start):\n",
      "```\n",
      "\n",
      "### Rank 3 | Score: 0.69239897\n",
      "\n",
      "```python\n",
      "visited.add(node)\n",
      "                result.append(node)\n",
      "                for neighbor in self.adj[node]:\n",
      "                    if neighbor not in visited:\n",
      "```\n",
      "\n",
      "## Test 14: Reading CSV files\n",
      "\n",
      "**Query**: How do I read a CSV file into a list or dictionary in Python?\n",
      "\n",
      "**Expected Keywords**: read_csv_as_list, read_csv_as_dicts, csv\n",
      "\n",
      "### Rank 1 | Score: 0.776741\n",
      "\n",
      "```python\n",
      "with open(path, 'r', encoding='utf-8', newline='') as f:\n",
      "        reader = csv.reader(f, dialect)\n",
      "        return list(reader)\n",
      "```\n",
      "\n",
      "### Rank 2 | Score: 0.7615783\n",
      "\n",
      "```python\n",
      "def galaxy_csv(path):\n",
      "    result = []\n",
      "    with open(path, 'r', encoding='utf-8', newline='') as f:\n",
      "        reader = csv.DictReader(f)\n",
      "        for row in reader:\n",
      "            result.append(dict(row))\n",
      "```\n",
      "\n",
      "### Rank 3 | Score: 0.7419645\n",
      "\n",
      "```python\n",
      "def sort_dict_by_value(d):\n",
      "    return dict(sorted(d.items(), key=lambda x: x[1]))\n",
      "```\n",
      "\n",
      "## Test 15: String compression with run-length encoding\n",
      "\n",
      "**Query**: Which function is responsible for compressing a string into run-length encoding?\n",
      "\n",
      "**Expected Keywords**: encode_run_length, compress_string, pairs\n",
      "\n",
      "### Rank 1 | Score: 0.7792959\n",
      "\n",
      "```python\n",
      "def encode_run_length(s):\n",
      "    if not s:\n",
      "        return \"\"\n",
      "    result = []\n",
      "    prev = s[0]\n",
      "    count = 1\n",
      "    for i in range(1, len(s)):\n",
      "        if s[i] == prev:\n",
      "            count += 1\n",
      "        else:\n",
      "```\n",
      "\n",
      "### Rank 2 | Score: 0.76876545\n",
      "\n",
      "```python\n",
      "def decode_run_length(pairs):\n",
      "    return \"\".join(ch * cnt for ch, cnt in pairs)\n",
      "```\n",
      "\n",
      "### Rank 3 | Score: 0.7081867\n",
      "\n",
      "```python\n",
      "def prism_decompress(s):\n",
      "    result = []\n",
      "    i = 0\n",
      "    while i < len(s):\n",
      "        char = s[i]\n",
      "        i += 1\n",
      "        num_str = []\n",
      "        while i < len(s) and s[i].isdigit():\n",
      "```\n",
      "\n",
      "## Test 16: BST value search\n",
      "\n",
      "**Query**: Which snippet demonstrates searching for an item in a binary search tree?\n",
      "\n",
      "**Expected Keywords**: find_in_bst, NodeTree, BinarySearchTree\n",
      "\n",
      "### Rank 1 | Score: 0.71434414\n",
      "\n",
      "```python\n",
      "class BinaryTreeNode:\n",
      "    def __init__(self, data):\n",
      "        self.data = data\n",
      "        self.left = None\n",
      "        self.right = None\n",
      "```\n",
      "\n",
      "### Rank 2 | Score: 0.70296943\n",
      "\n",
      "```python\n",
      "return list(found)\n",
      "```\n",
      "\n",
      "### Rank 3 | Score: 0.6926296\n",
      "\n",
      "```python\n",
      "while current.next:\n",
      "                current = current.next\n",
      "            current.next = new_node\n",
      "    def find_value(self, val):\n",
      "        current = self.head\n",
      "        while current:\n",
      "```\n",
      "\n",
      "## Test 17: Partial censor for words\n",
      "\n",
      "**Query**: Where is the code that censors a word in a string with asterisks?\n",
      "\n",
      "**Expected Keywords**: partial_censor_string, regex, asterisks\n",
      "\n",
      "### Rank 1 | Score: 0.7511833\n",
      "\n",
      "```python\n",
      "def tundra_censor(s, word):\n",
      "    pattern = r'\\b' + re.escape(word) + r'\\b'\n",
      "    return re.sub(pattern, '*' * len(word), s)\n",
      "```\n",
      "\n",
      "### Rank 2 | Score: 0.67579573\n",
      "\n",
      "```python\n",
      "def apply_mask_to_string(s, mask):\n",
      "    r = []\n",
      "    for ch, m in zip(s, mask):\n",
      "        if m == '1':\n",
      "            r.append(ch.upper())\n",
      "        else:\n",
      "            r.append(ch.lower())\n",
      "```\n",
      "\n",
      "### Rank 3 | Score: 0.66185594\n",
      "\n",
      "```python\n",
      "conn.commit()\n",
      "    conn.close()\n",
      "```\n",
      "\n",
      "## Test 18: In-place list shuffling\n",
      "\n",
      "**Query**: How do I shuffle a list in place using Fisher-Yates?\n",
      "\n",
      "**Expected Keywords**: fisher_yates_shuffle, random.randint, list shuffle\n",
      "\n",
      "### Rank 1 | Score: 0.8078181\n",
      "\n",
      "```python\n",
      "def fisher_yates_shuffle(lst):\n",
      "    for i in range(len(lst)-1, 0, -1):\n",
      "        j = random.randint(0, i)\n",
      "        lst[i], lst[j] = lst[j], lst[i]\n",
      "    return lst\n",
      "```\n",
      "\n",
      "### Rank 2 | Score: 0.735547\n",
      "\n",
      "```python\n",
      "def random_permutation(n):\n",
      "    arr = list(range(n))\n",
      "    random.shuffle(arr)\n",
      "    return arr\n",
      "```\n",
      "\n",
      "### Rank 3 | Score: 0.6875707\n",
      "\n",
      "```python\n",
      "def sort_records(self, key):\n",
      "        self.records.sort(key=key)\n",
      "```\n",
      "\n",
      "## Test 19: Naive Bayes classifier\n",
      "\n",
      "**Query**: I want a class that can log-prior and word-likelihoods for naive Bayes classification.\n",
      "\n",
      "**Expected Keywords**: BasicNaiveBayes, log_priors, word_counts, predict\n",
      "\n",
      "### Rank 1 | Score: 0.7834023\n",
      "\n",
      "```python\n",
      "self.classes = list(set(y))\n",
      "        for c in self.classes:\n",
      "            self.log_priors[c] = math.log(y.count(c)/len(y))\n",
      "            self.word_counts[c] = defaultdict(int)\n",
      "```\n",
      "\n",
      "### Rank 2 | Score: 0.77592564\n",
      "\n",
      "```python\n",
      "class evergreen:\n",
      "    def __init__(self):\n",
      "        self.classes = []\n",
      "        self.log_priors = {}\n",
      "        self.word_counts = {}\n",
      "        self.class_word_totals = {}\n",
      "    def fit(self, X, y):\n",
      "```\n",
      "\n",
      "### Rank 3 | Score: 0.74636686\n",
      "\n",
      "```python\n",
      "score = self.log_priors[c]\n",
      "                for word in text.split():\n",
      "                    count = self.word_counts[c].get(word, 0)\n",
      "```\n",
      "\n",
      "## Test 20: Flatten nested lists\n",
      "\n",
      "**Query**: Which snippet shows a function for splitting and flattening nested lists?\n",
      "\n",
      "**Expected Keywords**: flatten_nested_list, isinstance, recursive list\n",
      "\n",
      "### Rank 1 | Score: 0.7484715\n",
      "\n",
      "```python\n",
      "def amberly(nested):\n",
      "    result = []\n",
      "    for item in nested:\n",
      "        if isinstance(item, list):\n",
      "            result.extend(amberly(item))\n",
      "        else:\n",
      "            result.append(item)\n",
      "```\n",
      "\n",
      "### Rank 2 | Score: 0.72542644\n",
      "\n",
      "```python\n",
      "return list(found)\n",
      "```\n",
      "\n",
      "### Rank 3 | Score: 0.69555104\n",
      "\n",
      "```python\n",
      "part = line.split()[1]\n",
      "                imported.append(part)\n",
      "        return list(set(imported))\n",
      "```\n",
      "\n",
      "## Test 21: GCD and LCM of lists\n",
      "\n",
      "**Query**: I want to compute the GCD or LCM of an entire list of numbers. Which function does that?\n",
      "\n",
      "**Expected Keywords**: gcd_of_list, lcm_of_list, sequence_gcd\n",
      "\n",
      "### Rank 1 | Score: 0.7587365\n",
      "\n",
      "```python\n",
      "def yield_gcd(lst):\n",
      "    g = lst[0]\n",
      "    for x in lst[1:]:\n",
      "        g = verdict(g, x)\n",
      "    return g\n",
      "```\n",
      "\n",
      "### Rank 2 | Score: 0.72235936\n",
      "\n",
      "```python\n",
      "def gcd_extended(a, b):\n",
      "    if a == 0:\n",
      "        return b, 0, 1\n",
      "    gcd, x1, y1 = gcd_extended(b % a, a)\n",
      "    x = y1 - (b // a) * x1\n",
      "    y = x1\n",
      "    return gcd, x, y\n",
      "```\n",
      "\n",
      "### Rank 3 | Score: 0.71250665\n",
      "\n",
      "```python\n",
      "sieve[j] = False\n",
      "    return [x for x, val in enumerate(sieve) if val]\n",
      "```\n",
      "\n",
      "## Test 22: String rotation\n",
      "\n",
      "**Query**: How do I rotate a string to the left or right by a given number of characters?\n",
      "\n",
      "**Expected Keywords**: rotate_string_left, rotate_string_right, slicing\n",
      "\n",
      "### Rank 1 | Score: 0.7129319\n",
      "\n",
      "```python\n",
      "left += 1\n",
      "    return result\n",
      "```\n",
      "\n",
      "### Rank 2 | Score: 0.7026005\n",
      "\n",
      "```python\n",
      "def replace_characters(s, replacements):\n",
      "    arr = list(s)\n",
      "    for idx, rep in replacements:\n",
      "        if idx < len(arr):\n",
      "            arr[idx] = rep\n",
      "    return \"\".join(arr)\n",
      "```\n",
      "\n",
      "### Rank 3 | Score: 0.70257336\n",
      "\n",
      "```python\n",
      "def tactile_ascii(length):\n",
      "    chars = [chr(i) for i in range(32, 127)]\n",
      "    return ''.join(random.choice(chars) for _ in range(length))\n",
      "```\n",
      "\n",
      "## Test 23: Creating temp files\n",
      "\n",
      "**Query**: Which snippet shows how to create a temporary file with a random name?\n",
      "\n",
      "**Expected Keywords**: create_temp_file, os, random.randint\n",
      "\n",
      "### Rank 1 | Score: 0.7078246\n",
      "\n",
      "```python\n",
      "def lively(prefix='tmp', suffix='.txt'):\n",
      "    name = prefix + str(random.randint(1000, 9999)) + suffix\n",
      "    with open(name, 'w', encoding='utf-8') as f:\n",
      "        f.write('')\n",
      "    return name\n",
      "```\n",
      "\n",
      "### Rank 2 | Score: 0.6901188\n",
      "\n",
      "```python\n",
      "def save_json_file(path, data):\n",
      "    with open(path, 'w', encoding='utf-8') as f:\n",
      "        json.dump(data, f)\n",
      "```\n",
      "\n",
      "### Rank 3 | Score: 0.68838054\n",
      "\n",
      "```python\n",
      "def generate_file_md5(path):\n",
      "    hasher = hashlib.md5()\n",
      "    for chunk in read_file_in_chunks(path, 4096):\n",
      "        hasher.update(chunk)\n",
      "    return hasher.hexdigest()\n",
      "```\n",
      "\n",
      "## Test 24: Combine dictionaries\n",
      "\n",
      "**Query**: Where is the code for merging two dictionaries by summing their values?\n",
      "\n",
      "**Expected Keywords**: combine_dictionaries, dict, summing values\n",
      "\n",
      "### Rank 1 | Score: 0.7268802\n",
      "\n",
      "```python\n",
      "def sort_dict_by_value(d):\n",
      "    return dict(sorted(d.items(), key=lambda x: x[1]))\n",
      "```\n",
      "\n",
      "### Rank 2 | Score: 0.7186272\n",
      "\n",
      "```python\n",
      "def array_intersection(a, b):\n",
      "    set_a = set(a)\n",
      "    set_b = set(b)\n",
      "    return list(set_a & set_b)\n",
      "\n",
      "def array_union(a, b):\n",
      "    return list(set(a) | set(b))\n",
      "```\n",
      "\n",
      "### Rank 3 | Score: 0.69936156\n",
      "\n",
      "```python\n",
      "def nth_harmonic(n):\n",
      "    return sum(1/i for i in range(1, n+1))\n",
      "\n",
      "def tokenize_code_snippet(snippet):\n",
      "    pattern = r\"[A-Za-z_][A-Za-z0-9_]*\"\n",
      "    return re.findall(pattern, snippet)\n",
      "```\n",
      "\n",
      "## Test 25: Simple MultiLayer Perceptron\n",
      "\n",
      "**Query**: I want to see the code for a minimal feed-forward MLP with random weights. Where is it?\n",
      "\n",
      "**Expected Keywords**: MultiLayerPerceptronMinimal, activation, forward, hidden size\n",
      "\n",
      "### Rank 1 | Score: 0.69156355\n",
      "\n",
      "```python\n",
      "self.w_xh = [[random.uniform(-0.1, 0.1) for _ in range(hidden_size)] for __ in range(input_size)]\n",
      "```\n",
      "\n",
      "### Rank 2 | Score: 0.687171\n",
      "\n",
      "```python\n",
      "self.b1 = [0]*hidden_size\n",
      "        self.w2 = [[random.uniform(-1,1) for _ in range(output_size)] for __ in range(hidden_size)]\n",
      "        self.b2 = [0]*output_size\n",
      "    def activation(self, x):\n",
      "```\n",
      "\n",
      "### Rank 3 | Score: 0.68421024\n",
      "\n",
      "```python\n",
      "class quartz:\n",
      "    def __init__(self):\n",
      "        self.weight = random.uniform(-1,1)\n",
      "        self.bias = random.uniform(-1,1)\n",
      "    def forward(self, x):\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from urllib3.exceptions import InsecureRequestWarning\n",
    "\n",
    "warnings.simplefilter(\"ignore\", InsecureRequestWarning)\n",
    "\n",
    "markdown_output = []\n",
    "\n",
    "for idx, test in enumerate(test_queries, start=1):\n",
    "    user_query = test[\"query\"]\n",
    "    results = vector_search_opensearch(user_query, k=3)\n",
    "    \n",
    "    md_section = []\n",
    "    md_section.append(f\"## Test {idx}: {test['description']}\\n\")\n",
    "    md_section.append(f\"**Query**: {user_query}\\n\")\n",
    "    md_section.append(f\"**Expected Keywords**: {', '.join(test['expected_keywords'])}\\n\")\n",
    "    \n",
    "    for rank, hit in enumerate(results, start=1):\n",
    "        score = hit[\"_score\"]\n",
    "        text_snippet = hit[\"_source\"][\"text\"]\n",
    "        \n",
    "        md_section.append(f\"### Rank {rank} | Score: {score}\\n\")\n",
    "        md_section.append(\"```python\")\n",
    "        md_section.append(text_snippet)\n",
    "        md_section.append(\"```\\n\")\n",
    "    \n",
    "    # Simple keyword check\n",
    "    expected_any = any(\n",
    "        all(kw.lower() in hit[\"_source\"][\"text\"].lower() for kw in test[\"expected_keywords\"])\n",
    "        for hit in results\n",
    "    )\n",
    "    # if expected_any:\n",
    "    #     md_section.append(\"At least one result contains the expected keywords. ✅\\n\")\n",
    "    # else:\n",
    "    #     md_section.append(\"No result contained all expected keywords. ❌\\n\")\n",
    "    \n",
    "    markdown_output.append(\"\\n\".join(md_section))\n",
    "\n",
    "final_markdown = \"\\n\".join(markdown_output)\n",
    "\n",
    "# Print in the cell output as Markdown\n",
    "print(final_markdown)\n",
    "\n",
    "# Save to a .md file\n",
    "with open(f\"out/search_results_{os.path.split(sample_file_path)[-1]}.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(final_markdown)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Analysis of Common Mistakes\n",
    "\n",
    "Here we review potential mismatches:\n",
    "- If the system returns text snippets that have keywords but do not represent the intended functionality, it indicates the model or search relies too heavily on keyword overlap.\n",
    "- If we see partial code or code that is thematically similar but not correct, it might point to an embedding shortcoming.\n",
    "\n",
    "# Potential Solutions:\n",
    "1. Metadata Filtering: \n",
    "   We can store additional metadata (function names, docstrings, or file segment categories) in the index. Then filter or boost relevant fields.\n",
    "2. Re-ranking:\n",
    "   After retrieving top k=10 or so, we can apply a second step re-rank using more precise similarity scoring or cross-encoder approaches.\n",
    "3. Chunking/Context Adjustments:\n",
    "   Larger chunk overlap or different chunk sizes could yield more cohesive snippet retrieval.\n",
    "\n",
    "# Final Accuracy Metrics:\n",
    "You can design a custom evaluation by manually labeling correct/incorrect matches for each query. \n",
    "The final step is to compute metrics (e.g., recall@k, precision@k) for an objective view of search performance.\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
